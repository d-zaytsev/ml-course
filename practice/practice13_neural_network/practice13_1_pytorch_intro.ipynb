{"cells":[{"cell_type":"markdown","source":["# Введение в PyTorch\n","\n","В этом блокноте рассмотрим основную информацию, необходимую для реализации обучения нейронных сетей: тензоры, градиенты, слои нейронной сети, оптимизаторы."],"metadata":{"id":"zfUQUh3WbNS-"}},{"cell_type":"markdown","metadata":{"id":"jb0SsOz3Yo8s"},"source":["Тензоры\n","=======\n","\n","Тензоры - это специализированная структура данных, которая очень похожа на массивы и матрицы. В PyTorch используются тензоры для кодирования входных и выходных данных модели, а также ее параметров.\n","\n","Тензоры похожи на массивы NumPy, за исключением того, что тензоры могут работать на GPU или другом специализированном оборудовании для ускорения вычислений. Если вы знакомы с ndarrays, то легко привыкните к тензорам.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM15xRzcYo8t"},"outputs":[],"source":["%matplotlib inline\n","\n","import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"o6y5GkXSYo8t"},"source":["##Инициализация тензоров\n","\n","Тензоры можно инициализировать различными способами.\n","\n","**Непосредственно из данных**.\n","\n","Тензоры можно создавать непосредственно из данных, при этом тип данных определяется автоматически.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OH7P-PXYo8t"},"outputs":[],"source":["data = [[1, 2], [3, 4]]\n","x_data = torch.tensor(data)"]},{"cell_type":"markdown","metadata":{"id":"eOWyYRlcYo8u"},"source":["**Из массивов NumPy**\n","\n","Тензоры можно создавать из массивов NumPy и наоборот.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8TvqmNIYo8u"},"outputs":[],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"]},{"cell_type":"markdown","metadata":{"id":"rutdiTI4Yo8u"},"source":["**Из другого тензора**\n","\n","Новый тензор сохраняет свойства (shape, тип данных) аргумента\n","тензора, если он не был явно переопределен.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbaKz8jAYo8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609238,"user_tz":-180,"elapsed":16,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"4373fe5d-5211-4e00-b9c2-ef95cff492b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.1876, 0.2977],\n","        [0.8114, 0.3004]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"ViacvxaOYo8u"},"source":["**Случайным значением или константой:**\n","\n","`shape` - кортеж размерностей тензора. В приведенных ниже функциях он определяет размерность тензора.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQO-1BGMYo8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609239,"user_tz":-180,"elapsed":14,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"95fe15ef-4ee7-44cc-b327-dbd612ccdbed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.9112, 0.1112, 0.7517],\n","        [0.8409, 0.1488, 0.7270]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2, 3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"abbdJt6QYo8v"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"Gk4vMrvtYo8v"},"source":["##Атрибуты тензоров\n","\n","Атрибуты тензора описывают его `shape`, тип данных и устройство, на котором он хранится.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukofkH0SYo8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609239,"user_tz":-180,"elapsed":10,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"8cba741a-c45e-49f2-850a-13baaa571124"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3, 4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"6bhATCd5Yo8v"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"fNG3-imUYo8v"},"source":["##Операции над тензорами\n","\n","Доступны такие операции, как: транспонирование, индексирование, срезы,\n","математические операции, линейная алгебра, выборка и [многие другие](https://pytorch.org/docs/stable/torch.html).\n","\n","Каждая из них может быть выполнена на GPU (обычно с более высокой скоростью, чем на\n","CPU)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36AccQFpYo8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":654,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"2c123fd4-befa-45a3-8ed0-f0de024570c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device tensor is stored on: cuda:0\n"]}],"source":["# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","  tensor = tensor.to('cuda')\n","  print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"_KP-EOB2Yo8v"},"source":["**Индексирование и срезы:**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mii2aiLQYo8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":28,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"215d58d2-e2d2-4829-a1b4-5d40d530d8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","tensor[:,1] = 0\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"p2vI7h6FYo8w"},"source":["**Объединение тензоров** `torch.cat`, `torch.cat`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3JU_GLHYo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025661734,"user_tz":-180,"elapsed":335,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"bfc00398-4e54-41ac-ed99-e733b81c4bd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 4])\n","torch.Size([4, 12])\n","tensor([[6., 5., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6.],\n","        [6., 5., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6.],\n","        [6., 5., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6.],\n","        [6., 5., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6.]])\n","torch.Size([4, 3, 4])\n","tensor([[[6., 5., 6., 6.],\n","         [6., 5., 6., 6.],\n","         [6., 5., 6., 6.]],\n","\n","        [[6., 5., 6., 6.],\n","         [6., 5., 6., 6.],\n","         [6., 5., 6., 6.]],\n","\n","        [[6., 5., 6., 6.],\n","         [6., 5., 6., 6.],\n","         [6., 5., 6., 6.]],\n","\n","        [[6., 5., 6., 6.],\n","         [6., 5., 6., 6.],\n","         [6., 5., 6., 6.]]])\n"]}],"source":["print(tensor.shape)\n","t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1.shape)\n","print(t1)\n","t2 = torch.stack([tensor, tensor, tensor], dim=1)\n","print(t2.shape)\n","print(t2)"]},{"cell_type":"markdown","metadata":{"id":"C5AS1iapYo8w"},"source":["**Умножение тензоров**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3slnPvUYo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":22,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"c14d1082-4917-4bb3-af37-b62469fda933"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor.mul(tensor) \n"," tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor * tensor \n"," tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["# This computes the element-wise product\n","print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n","# Alternative syntax:\n","print(f\"tensor * tensor \\n {tensor * tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"_81o79qMYo8w"},"source":["Матричное умножение двух тензоров.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-jikhF4Yo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":19,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"c35534bf-53ca-49eb-f84d-12ba3ce38c48"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor.matmul(tensor.T) \n"," tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]]) \n","\n","tensor @ tensor.T \n"," tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n"]}],"source":["print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n","# Alternative syntax:\n","print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"]},{"cell_type":"markdown","metadata":{"id":"pX_WWO4RYo8w"},"source":["**In-place операции** с суффиксом `_`: `x.copy_(y)`, `x.t_()`, изменят содержимое `x`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SInjZquAYo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":16,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"b454e75a-cbdc-41ed-9d33-908637f0eae9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor([[6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.]])\n"]}],"source":["print(tensor, \"\\n\")\n","tensor.add_(5)\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"Tf45K3WIYo8w"},"source":["<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n","<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n","<p>In-place операции экономят немного памяти, но могут быть проблематичны при вычислении градиентов из-за потери истории. Поэтому их использование не рекомендуется.</p>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"JD1kb1eZYo8w"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"_7aD3JE-Yo8w"},"source":["##Совместимость с NumPy\n","\n","Тензоры на CPU и массивы NumPy могут совместно использовать свои базовые области памяти и изменение одного из них приведет к изменению другого.\n"]},{"cell_type":"markdown","metadata":{"id":"AxiXhvyZYo8w"},"source":["##Tensor to NumPy array\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nygeGx57Yo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":13,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"4ae154a5-034d-4c4d-b75a-b9216d7052bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"QccbweviYo8w"},"source":["Изменения тензора повлекут изменения массива NumPy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oeINbsIYo8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":9,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"f61d86bc-33a4-47e9-83ec-d6e783c09672"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"LQOzYbwUYo8w"},"source":["##NumPy array to Tensor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSkuufTvYo8w"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"6e5Mcu0PYo8w"},"source":["Изменения массива NumPy повлекут изменения тензора.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeazEnQfYo8x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728025609887,"user_tz":-180,"elapsed":6,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"5f260751-3fde-400d-d588-78802d64e6a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"L0q-yIT0qu4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTYsZJlWY2Eu"},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"pJeXSu8hY2Ev"},"source":["Введение в `torch.autograd`\n","===========================\n","\n","`torch.autograd` - это движок автоматического дифференцирования PyTorch, который обеспечивает обучение нейронных сетей.\n","\n","Нейронные сети\n","--------------\n","\n","Нейронные сети (НС) - это набор вложенных функций, которые выполняются на некоторых входных данных. Эти функции определяются *параметрами*\n","(состоящими из весов (weights) и смещений (biases)), которые в PyTorch хранятся в виде тензоров.\n","\n","Обучение НС происходит в два этапа:\n","\n","**Forward Propagation**: При прямом распространении НС делает свое лучшее предположение о правильном выходе. Он прогоняет входные данные через каждую из своих функции, чтобы сделать это предположение.\n","\n","**Backward Propagation**: При обратном распространении НС изменяет свои параметры пропорционально ошибке в своем предположении. Для этого он проходит\n","назад от выхода, собирая производные ошибок относительно параметров функций, и оптимизирует параметры с помощью градиентного спуска.\n","\n","Использование в PyTorch\n","-----------------------\n","\n","Давайте рассмотрим один шаг обучения. Для этого примера мы загрузим\n","предварительно обученную модель resnet18 из `torchvision`. Мы создаем случайный тензор для представления одного изображения с 3 каналами, высотой и шириной по 64, и соответствующие `label`, инициализированные некоторыми случайными значениями.\n","Метки в предварительно обученных моделях имеет форму (1,1000).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YSRx0dPY2Ew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728031634713,"user_tz":-180,"elapsed":12656,"user":{"displayName":"Rustam Azimov","userId":"04814656827348772230"}},"outputId":"ac541a1a-fc3f-4da8-c110-2ae884aa6628"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 110MB/s]\n"]}],"source":["import torch\n","from torchvision.models import resnet18, ResNet18_Weights\n","model = resnet18(weights=ResNet18_Weights.DEFAULT)\n","data = torch.rand(1, 3, 64, 64)\n","labels = torch.rand(1, 1000)"]},{"cell_type":"markdown","metadata":{"id":"pjsOpuLgY2Ex"},"source":["**Forward pass** - прогоним входные данные через все слои модели, чтобы сделать предсказание.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-klENqIZY2Ex"},"outputs":[],"source":["prediction = model(data) # forward pass"]},{"cell_type":"markdown","metadata":{"id":"wC8nQHzeY2Ex"},"source":["Используем предсказание модели и истинные значения `label`, чтобы вычислить ошибку (`loss`). **Backward propagation** запускается когда мы вызываем\n","`.backward()` у тензора с ошибками. Autograd вычисляет и сохраняет градиенты для кажого параметра модели в его атрибуте `.grad`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zF-HnetLY2Ex"},"outputs":[],"source":["loss = (prediction - labels).sum()\n","loss.backward() # backward pass"]},{"cell_type":"markdown","metadata":{"id":"o_C9wYpaY2Ex"},"source":["Далее мы загружаем оптимизатор (например, SGD) с нужным learning rate (например, 0.01) и [momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n","0.9. Фиксируем все параметры модели в оптимизаторе.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02zfr5G6Y2Ey"},"outputs":[],"source":["optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"gEbKPycqY2Ey"},"source":["Осталось вызвать метод `.step()` для запуска шага градиентного спуска. Оптимизатор изменит каждый параметр в соответсвии с их градиентом, сохраненном в `.grad`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNZzWMorY2Ey"},"outputs":[],"source":["optim.step() #gradient descent"]},{"cell_type":"markdown","metadata":{"id":"6hKI0I4SY2Ey"},"source":["Этого достаточно, чтобы самостоятельно реализовать обучение нейронной сети.\n"]},{"cell_type":"markdown","metadata":{"id":"4VsVM8z-BJLu"},"source":["Neural Networks\n","===============\n","\n","Neural networks can be constructed using the `torch.nn` package.\n","\n","Now that you had a glimpse of `autograd`, `nn` depends on `autograd` to\n","define models and differentiate them. An `nn.Module` contains layers,\n","and a method `forward(input)` that returns the `output`.\n","\n","For example, look at this network that classifies digit images:\n","\n","![convnet](https://pytorch.org/tutorials/_static/img/mnist.png)\n","\n","It is a simple feed-forward network. It takes the input, feeds it\n","through several layers one after the other, and then finally gives the\n","output.\n","\n","A typical training procedure for a neural network is as follows:\n","\n","-   Define the neural network that has some learnable parameters (or\n","    weights)\n","-   Iterate over a dataset of inputs\n","-   Process input through the network\n","-   Compute the loss (how far is the output from being correct)\n","-   Propagate gradients back into the network's parameters\n","-   Update the weights of the network, typically using a simple update\n","    rule: `weight = weight - learning_rate * gradient`\n","\n","##Define the network\n","\n","Let's define this network:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkJA6zUjBJLv"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 5x5 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, input):\n","        # Convolution layer C1: 1 input image channel, 6 output channels,\n","        # 5x5 square convolution, it uses RELU activation function, and\n","        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n","        c1 = F.relu(self.conv1(input))\n","        # Subsampling layer S2: 2x2 grid, purely functional,\n","        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n","        s2 = F.max_pool2d(c1, (2, 2))\n","        # Convolution layer C3: 6 input channels, 16 output channels,\n","        # 5x5 square convolution, it uses RELU activation function, and\n","        # outputs a (N, 16, 10, 10) Tensor\n","        c3 = F.relu(self.conv2(s2))\n","        # Subsampling layer S4: 2x2 grid, purely functional,\n","        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n","        s4 = F.max_pool2d(c3, 2)\n","        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n","        s4 = torch.flatten(s4, 1)\n","        # Fully connected layer F5: (N, 400) Tensor input,\n","        # and outputs a (N, 120) Tensor, it uses RELU activation function\n","        f5 = F.relu(self.fc1(s4))\n","        # Fully connected layer F6: (N, 120) Tensor input,\n","        # and outputs a (N, 84) Tensor, it uses RELU activation function\n","        f6 = F.relu(self.fc2(f5))\n","        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n","        # outputs a (N, 10) Tensor\n","        output = self.fc3(f6)\n","        return output\n","\n","\n","net = Net()\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"aVrY_-fTBJLw"},"source":["You just have to define the `forward` function, and the `backward`\n","function (where gradients are computed) is automatically defined for you\n","using `autograd`. You can use any of the Tensor operations in the\n","`forward` function.\n","\n","The learnable parameters of a model are returned by `net.parameters()`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu89QBlgBJLw"},"outputs":[],"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1's .weight"]},{"cell_type":"markdown","metadata":{"id":"L-FOaByoBJLw"},"source":["Let\\'s try a random 32x32 input. Note: expected input size of this net\n","(LeNet) is 32x32. To use this net on the MNIST dataset, please resize\n","the images from the dataset to 32x32.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GU_QozuRBJLw"},"outputs":[],"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)"]},{"cell_type":"markdown","metadata":{"id":"AgeJ8g2SBJLx"},"source":["Zero the gradient buffers of all parameters and backprops with random\n","gradients:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VORZ02eSBJLx"},"outputs":[],"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"]},{"cell_type":"markdown","metadata":{"id":"c8yKHhvvZDIP"},"source":["<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n","<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n","<p><code>torch.nn</code> only supports mini-batches. The entire <code>torch.nn</code>package only supports inputs that are a mini-batch of samples, and nota single sample.For example, <code>nn.Conv2d</code> will take in a 4D Tensor of<code>nSamples x nChannels x Height x Width</code>.If you have a single sample, just use <code>input.unsqueeze(0)</code> to adda fake batch dimension.</p>\n","</div>\n","\n","Before proceeding further, let\\'s recap all the classes you've seen so\n","far.\n","\n","**Recap:**\n","\n",":   -   `torch.Tensor` - A *multi-dimensional array* with support for\n","        autograd operations like `backward()`. Also *holds the gradient*\n","        w.r.t. the tensor.\n","    -   `nn.Module` - Neural network module. *Convenient way of\n","        encapsulating parameters*, with helpers for moving them to GPU,\n","        exporting, loading, etc.\n","    -   `nn.Parameter` - A kind of Tensor, that is *automatically\n","        registered as a parameter when assigned as an attribute to a*\n","        `Module`.\n","    -   `autograd.Function` - Implements *forward and backward\n","        definitions of an autograd operation*. Every `Tensor` operation\n","        creates at least a single `Function` node that connects to\n","        functions that created a `Tensor` and *encodes its history*.\n","\n","**At this point, we covered:**\n","\n",":   -   Defining a neural network\n","    -   Processing inputs and calling backward\n","\n","**Still Left:**\n","\n",":   -   Computing the loss\n","    -   Updating the weights of the network\n","\n","##Loss Function\n","\n","A loss function takes the (output, target) pair of inputs, and computes\n","a value that estimates how far away the output is from the target.\n","\n","There are several different [loss\n","functions](https://pytorch.org/docs/nn.html#loss-functions) under the nn\n","package . A simple loss is: `nn.MSELoss` which computes the mean-squared\n","error between the output and the target.\n","\n","For example:\n"]},{"cell_type":"code","source":["output = net(input)\n","target = torch.randn(10)  # a dummy target, for example\n","target = target.view(1, -1)  # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"metadata":{"id":"iCp60129BtbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, if you follow `loss` in the backward direction, using its\n","`.grad_fn` attribute, you will see a graph of computations that looks\n","like this:\n","\n","``` {.sourceCode .sh}\n","input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n","      -> flatten -> linear -> relu -> linear -> relu -> linear\n","      -> MSELoss\n","      -> loss\n","```\n","\n","So, when we call `loss.backward()`, the whole graph is differentiated\n","w.r.t. the neural net parameters, and all Tensors in the graph that have\n","`requires_grad=True` will have their `.grad` Tensor accumulated with the\n","gradient.\n","\n","For illustration, let us follow a few steps backward:"],"metadata":{"id":"AyLX9eaRBzH4"}},{"cell_type":"code","source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"metadata":{"id":"0DNImH7vBzSR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8l8U66ZeZDIQ"},"source":["##Backprop\n","\n","To backpropagate the error all we have to do is to `loss.backward()`.\n","You need to clear the existing gradients though, else gradients will be\n","accumulated to existing gradients.\n","\n","Now we shall call `loss.backward()`, and have a look at conv1\\'s bias\n","gradients before and after the backward.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qx1C3cpVZDIQ"},"outputs":[],"source":["net.zero_grad()     # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"]},{"cell_type":"markdown","metadata":{"id":"ZeF-KpRHZDIQ"},"source":["Now, we have seen how to use loss functions.\n","\n","**Read Later:**\n","\n","> The neural network package contains various modules and loss functions\n","> that form the building blocks of deep neural networks. A full list\n","> with documentation is [here](https://pytorch.org/docs/nn).\n","\n","**The only thing left to learn is:**\n","\n","> -   Updating the weights of the network\n","\n","##Update the weights\n","\n","The simplest update rule used in practice is the Stochastic Gradient\n","Descent (SGD):\n","\n","``` {.sourceCode .python}\n","weight = weight - learning_rate * gradient\n","```\n","\n","We can implement this using simple Python code:\n","\n","``` {.sourceCode .python}\n","learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)\n","```\n","\n","However, as you use neural networks, you want to use various different\n","update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable\n","this, we built a small package: `torch.optim` that implements all these\n","methods. Using it is very simple:\n","\n","``` {.sourceCode .python}\n","import torch.optim as optim\n","\n","# create your optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# in your training loop:\n","optimizer.zero_grad()   # zero the gradient buffers\n","output = net(input)\n","loss = criterion(output, target)\n","loss.backward()\n","optimizer.step()    # Does the update\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"JjVTmNeiZDIQ"},"source":["<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n","<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n","<p>Observe how gradient buffers had to be manually set to zero using<code>optimizer.zero_grad()</code>.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/3dbbd6931d76adb0dc37d4e88b328852/tensor_tutorial.ipynb","timestamp":1727959106182}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}